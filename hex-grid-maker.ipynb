{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tobler.util import h3fy\n",
    "from tobler.area_weighted import area_interpolate\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LOR as GeoDataFrame and show\n",
    "plr = gpd.read_file(\"data/LOR_SHP_2019/Planungsraum_EPSG_25833.shp\")\n",
    "plr = plr.set_crs('EPSG:25833') # ETRS89 / UTM zone 33N\n",
    "plr.plot(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate hexgrid\n",
    "grid = h3fy(plr, resolution=8)\n",
    "grid.plot(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read population on LOR level and join with geo data\n",
    "df_pop = pd.read_csv(\n",
    "    \"data/EWR201912E_Matrix.csv\",\n",
    "    sep=\";\",\n",
    "    usecols=[\"RAUMID\", \"E_E\"],\n",
    "    dtype={\"RAUMID\": str}\n",
    ")\n",
    "plr = plr.join(df_pop.set_index(\"RAUMID\"), on=\"SCHLUESSEL\")\n",
    "plr.plot(\"E_E\", figsize=(12,12), legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing to Blocks\n",
    "Try to distribute exact population numbers to street block level \n",
    "where only rough categories are published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blocks = gpd.read_file(\"data/s_rbs_bloecke.json\")\n",
    "# remove uninhabited blocks\n",
    "blocks.drop(['id','bez','bezname','datum'], axis=1, inplace=True)\n",
    "blocks.drop(blocks[blocks.ewk == 'unbewohnt'].index, inplace=True)\n",
    "blocks.info()\n",
    "print(\"\\n\\nInhabitant categories\", blocks.ewk.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot LOR level and blocks together\n",
    "base = plr.plot(figsize=(25,25))\n",
    "blocks.plot(ax=base, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually check if blocks align with LOR\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3,ncols=2, figsize=(12,12))\n",
    "test_keys = [\"01011101\", \"01011102\",\"01011103\",\"01011104\", \"03040614\", \"09041403\"]\n",
    "\n",
    "for r in [0,1,2]:\n",
    "    for c in [0,1]: \n",
    "        key = test_keys[2*r + c];\n",
    "        ax = plr[plr.SCHLUESSEL == key].plot(ax=axes[r,c])\n",
    "        ax.set_axis_off()\n",
    "        blocks[blocks.plr==key]\\\n",
    "            .plot(ax=ax, color=\"red\",edgecolor=\"black\", alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual distribution. \n",
    "# Strategy: For every LOR\n",
    "# * find all blocks\n",
    "# * distribute all inhabitants\n",
    "#   * first the minimum on every block: 100-999 ==> 100\n",
    "#   * then the middle of each block : 100-999 ==> 500\n",
    "#   * then the rest equally into the 1000+ blocks\n",
    "# This is not exact but easy enough \n",
    "\n",
    "# Inhabitants Map\n",
    "im = {1000: \"mehr als 1.000 Einwohner\",\n",
    "                   100: \"100-999 Einwohner\",\n",
    "                   10: \"10-99 Einwohner\",\n",
    "                   1: \"1-9 Einwohner\"}\n",
    "\n",
    "blocks[\"num\"] = 0.0\n",
    "\n",
    "def distribute_inhabitants(plr_key):\n",
    "    inhabitants_left = plr[plr.SCHLUESSEL== plr_key].E_E.iloc[0]\n",
    "    blks = blocks[blocks.plr==plr_key]\n",
    "    \n",
    "    # For all blocks, add minimum of bucket\n",
    "    for b in [1,10,100,1000]:\n",
    "        nb = blks.ewk.value_counts().get(im[b], default=0)\n",
    "        if nb > 0:\n",
    "            blks.loc[blks.ewk == im[b], 'num'] = b\n",
    "            inhabitants_left -= nb * b\n",
    "        \n",
    "    # for small buckets, add mid\n",
    "    for b in [1,10,100]:\n",
    "        nb = blks.ewk.value_counts().get(im[b], default=0)\n",
    "        d = b * 4# new to be distributed\n",
    "        \n",
    "        # mid of bucket if enough left\n",
    "        if nb > 0 and nb*d < inhabitants_left:\n",
    "            blks.loc[blks.ewk == im[b], 'num'] += d\n",
    "            inhabitants_left -= nb*d\n",
    "    \n",
    "        # TODO: if not enough left, iterate through buckets until empty\n",
    "    \n",
    "    nb = blks.ewk.value_counts().get(im[1000], default=0)\n",
    "    if nb > 0:\n",
    "        d = np.floor(inhabitants_left / nb)\n",
    "        #print(inhabitants_left, d)\n",
    "        blks.loc[blks.ewk == im[1000], 'num'] += d\n",
    "        inhabitants_left -= nb*d\n",
    "        # last bit to first 1000+ bucket\n",
    "        # TODO: this doesn't work. value is not updated\n",
    "        # blks.loc[blks.ewk == im[1000],'num'].iat[0] += inhabitants_left\n",
    "    \n",
    "    # copy data to main df\n",
    "    blocks.loc[blocks.plr==plr_key, 'num'] = blks.num\n",
    "\n",
    "for key in plr.SCHLUESSEL:\n",
    "    try:\n",
    "        distribute_inhabitants(key)\n",
    "    except:\n",
    "        print('Problem with', key)\n",
    "\n",
    "# Check the error\n",
    "print(\"distributed\", blocks.num.sum())\n",
    "print(\"original\", plr.E_E.sum())\n",
    "print(\"error\",plr.E_E.sum() - blocks.num.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual check on block level\n",
    "blocks.plot(\"num\", figsize=(20,20), legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging with Hex\n",
    "Now that we have estimated inhabitants on block level\n",
    "we merge that info with the hex grid.\n",
    "\n",
    "We call the measure for how inhabited a grid cell is __weight__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default value 0 for weight -> uninhabited\n",
    "grid[\"weight\"] = 0.0\n",
    "\n",
    "# For every cell in grid\n",
    "for i,cell in grid.iterrows():\n",
    "    try:\n",
    "        # find all blocks that intersect\n",
    "        b = blocks[blocks.intersects(cell.geometry)]\n",
    "        # calculate intersection polygon\n",
    "        ib = blocks[blocks.intersects(cell.geometry)].intersection(cell.geometry)\n",
    "        \n",
    "        # weight is sum over all intersecting blocks\n",
    "        #   share of intersection divided by block size * inhabitants in block\n",
    "        grid.loc[i, \"weight\"] = (ib.area / b.area * b.num).sum()\n",
    "    except:\n",
    "        print(\"Problem with\", i)\n",
    "        \n",
    "# We need log of weight because very few blocks have very high number of inhabitants\n",
    "# We need log1p(x) = log(x+1) to avoid calculating log(0)\n",
    "grid[\"log_weight\"] = np.log1p(grid[\"weight\"])\n",
    "grid.plot(\"log_weight\", figsize=(20,20), legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Centers\n",
    "Now we add the test centers to our grid\n",
    "\n",
    "For grid cells with a test center, we set travel time to 0\n",
    "\n",
    "For grid cells in the direct neighborhood, we set travel time to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the data provided by https://test-to-go.berlin/\n",
    "places = pd.read_json(\"data/test-places-2021-04-25.json\")\n",
    "places = gpd.GeoDataFrame(places, geometry=gpd.points_from_xy(places.lng, places.lat))\n",
    "\n",
    "# Test-To-Go uses GPS coordinates. We need to transform it to the projection we use for our grid\n",
    "places = places.set_crs('epsg:4326') # GPS\n",
    "places = places.to_crs('EPSG:25833') # ETRS89 / UTM zone 33N\n",
    "\n",
    "#drop mobile test centers\n",
    "places = places[places.lat != 0]\n",
    "places.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid[\"travel\"] = float(\"inf\")\n",
    "\n",
    "r = grid.iloc[0].geometry.length / 6\n",
    "\n",
    "# This can probably be written without a loop if you know (geo)pandas better\n",
    "for i,cell in grid.iterrows():\n",
    "    if places.within(cell.geometry).any():\n",
    "        grid.loc[i,\"travel\"] = 0.0\n",
    "\n",
    "for i,cell in grid.iterrows():\n",
    "    if np.isinf(cell.travel):\n",
    "        if ((grid.distance(cell.geometry) < r/2) & (grid.travel < 1.0)).any():\n",
    "            grid.loc[i,\"travel\"] = 1.0\n",
    "            \n",
    "grid.plot(\"travel\", figsize=(20,20), categorical= True, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travel Time To Test Centers\n",
    "For grid cells where we don't have a test center,\n",
    "we calculate the travel time with public transport to the nearest cell with a center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid recalculating the travel times every time, we save/load them to csv\n",
    "travel_times = pd.read_csv('data/travel_times.csv', \n",
    "                           index_col=[0,1], squeeze=True,\n",
    "                           header = 1, names=['from','to','seconds'])\n",
    "travel_times = travel_times.reindex(\n",
    "    index = pd.MultiIndex.from_product([grid.index, grid.index], names= ['from', 'to']),\n",
    "    fill_value = np.NaN\n",
    ")\n",
    "travel_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a hack to skip the execution of this cell \n",
    "# when the travel times were loaded from CSV\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "if 'travel_times' in globals():\n",
    "    print('Skipping because travel times have been loaded from CSV')\n",
    "    raise StopExecution\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import dateutil\n",
    "import datetime\n",
    "import calendar\n",
    "from ratelimiter import RateLimiter\n",
    "import sys\n",
    "\n",
    "# Set departure to next monday\n",
    "# because APIs don't allow requests too far away from today\n",
    "departure = (\n",
    "    datetime.date.today() + dateutil.relativedelta.relativedelta(weekday=calendar.MONDAY, hour=9)\n",
    ").astimezone().isoformat()\n",
    "departure\n",
    "\n",
    "num_test_travel = 5\n",
    "\n",
    "travel_times = pd.Series(\n",
    "    dtype= 'float64',\n",
    "    index=pd.MultiIndex.from_product([grid.index, grid.index], names= ['from', 'to'])\n",
    ")\n",
    "    \n",
    "url_base = \"https://v5.vbb.transport.rest/journeys\"\n",
    "\n",
    "# Using RateLimiter to stay within the limits the API expects\n",
    "@RateLimiter(max_calls=100, period=60)\n",
    "def calc_travel(fr,to):\n",
    "    \n",
    "    if not np.isnan(travel_times[fr.name, to.name]):\n",
    "        # already calculated\n",
    "        return\n",
    "    try:\n",
    "        #the API works with GPS coordinates so we have to transform from our coordinate system\n",
    "        #Using the centers of grid cells for travel time calculation\n",
    "        fr_gps = gpd.GeoSeries(fr.geometry.centroid).set_crs('EPSG:25833').to_crs('epsg:4326').iloc[0]\n",
    "        to_gps = gpd.GeoSeries(to.geometry.centroid).set_crs('EPSG:25833').to_crs('epsg:4326').iloc[0]\n",
    "\n",
    "        params = {\n",
    "            \"from.longitude\": fr_gps.x,\n",
    "            \"from.latitude\": fr_gps.y,\n",
    "            # API requires an address but doesn't use it if coordinates are present\n",
    "            \"from.address\": \"dummy_address\", \n",
    "            \"to.longitude\": to_gps.x,\n",
    "            \"to.latitude\": to_gps.y,\n",
    "            \"to.address\": \"dummy_address\",\n",
    "            \"departure\": departure,\n",
    "            \"results\": 1,\n",
    "        }\n",
    "        res = requests.get(url_base, params=params).json()\n",
    "        dep = dateutil.parser.parse(departure).timestamp()\n",
    "        \n",
    "        # Calculating and hoping the JSON is completely there\n",
    "        # \n",
    "        arr = dateutil.parser.parse(res['journeys'][0]['legs'][-1]['plannedArrival']).timestamp()\n",
    "\n",
    "        tt = (arr-dep)\n",
    "        travel_times[fr.name, to.name] = tt\n",
    "    # http://wiki.c2.com/?PokemonExceptionHandling\n",
    "    except: \n",
    "        print('Problem calculating travel time at', fr.name, to.name)\n",
    "        print()\n",
    "\n",
    "\n",
    "cnt = 0;\n",
    "total = grid.loc[np.isinf(grid.travel)].size\n",
    "for _,cFrom in grid.iterrows():\n",
    "    if np.isfinite(cFrom.travel):\n",
    "        continue\n",
    "    # We can't calculate the travel time to all test centers\n",
    "    # So we assume that the geographically closest n=5 grid cells\n",
    "    # are also the fastest by travel time\n",
    "    likely_nearest = grid.loc[\n",
    "        grid.loc[grid.travel == 0].distance(cFrom.geometry)\\\n",
    "            .sort_values().head(num_test_travel).index.values\n",
    "    ]\n",
    "    \n",
    "    for _, cTo in likely_nearest.iterrows():\n",
    "        calc_travel(cFrom, cTo)\n",
    "    \n",
    "    # for progress tracking\n",
    "    cnt += 1\n",
    "    if (cnt % 100 == 0):\n",
    "        print (cnt)\n",
    "\n",
    "travel_times.dropna().to_csv('data/travel_times.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge grid with travel times\n",
    "min_travel = grid.apply(lambda r: {'travel':travel_times[r.name].min(),\n",
    "                                   'to':travel_times[r.name].idxmin()}, \n",
    "                        axis=1, result_type='expand').dropna()\n",
    "# drop unrealistic values\n",
    "min_travel = min_travel[min_travel.travel < 8000]\n",
    "\n",
    "#add travel time to grid\n",
    "grid.loc[min_travel.index, 'travel'] = min_travel['travel'] / 60\n",
    "grid.loc[min_travel.index, 'to'] = min_travel['to']\n",
    "\n",
    "#tag error cases\n",
    "grid.loc[np.isinf(grid.travel), 'error_case'] = 'NOT_CALCULATED'\n",
    "grid.loc[grid.weight == 0, 'error_case'] = 'NOT_INHABITED'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "Now we combine number of inhabitants with travel time to the nearest test center\n",
    "for some kind of \"fairness measure\" or a visualization where test centers are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate value\n",
    "grid.loc[grid.error_case.isna() & (grid.travel > 1), 'value'] = np.log1p(grid.travel / grid.log_weight)\n",
    "grid.loc[grid.travel <= 1, 'value'] = 0\n",
    "\n",
    "# The plot\n",
    "cmap = plt.get_cmap('plasma', 10)\n",
    "ax = grid[grid.error_case.isna()].plot(\"value\", figsize=(20,20), legend=True, cmap='plasma')\n",
    "places.plot(ax=ax, color='red', markersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result with gps coordinates to geojson for further visualisation\n",
    "grid.to_crs('epsg:4326').to_file('docs/result.json', driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
